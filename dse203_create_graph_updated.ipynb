{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pakages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import neo4j\n",
    "import py2neo\n",
    "from pprint import pprint\n",
    "\n",
    "from py2neo import Database\n",
    "from py2neo import Graph, Node, Relationship, NodeMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connet neo4j database. \n",
    "HOST='localhost'\n",
    "USER='neo4jproj'# Set your own username.\n",
    "PW='1234' # Set your own password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(data:dict,key:str) -> str:\n",
    "    \n",
    "    ''' Get values from dictionary, \n",
    "        if key not in dictionary return None '''\n",
    "        \n",
    "    if key == 'Founder':\n",
    "        if key not in data.keys():\n",
    "            new_key='Founder(s)'\n",
    "            if new_key not in data.keys():\n",
    "                return None \n",
    "            else:\n",
    "                return data[new_key]\n",
    "        \n",
    "    elif key not in data.keys():\n",
    "        return None\n",
    "        \n",
    "    elif data[key]=='':\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return data[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquisition for all companies.\n",
    "\n",
    "Created a csv file with company acquisition, year and amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictiionary for acquisition. {company:{acquired_company:[year, amount]}...}\n",
    "acquisition={}\n",
    "with open('./info_box/Acquisition.csv') as Acquisition:\n",
    "    acquisition_all = csv.DictReader(Acquisition)\n",
    "    for row in acquisition_all:\n",
    "        if row['Company'] in acquisition.keys():\n",
    "            acquisition[row['Company']][row['Acquisition']]= [get_val(row,'Year'),get_val(row,'Amount')]\n",
    "        else:\n",
    "            acquisition[row['Company']]={}\n",
    "            acquisition[row['Company']][row['Acquisition']] = [get_val(row,'Year'),get_val(row,'Amount')]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alibaba': {'Alibaba Pictures': ['2014', ' $805 million'],\n",
      "             'Amblin Partners': ['2016', None],\n",
      "             'Ant Financial': ['2014', None],\n",
      "             'South China Morning Post': ['2015', '$266 million '],\n",
      "             'Vendio': ['2010', None]},\n",
      " 'Amazon': {'Ring Inc.': ['2018', '$1000 million'],\n",
      "            'Whole Foods Market': ['2017', '$13700 million'],\n",
      "            'Zappos': ['2009', '$1200 million ']},\n",
      " 'Walmart': {'Jet.com': ['2016', '$3000 million'],\n",
      "             'Shoes.com': ['1993', '$70 million'],\n",
      "             'Vudu': ['2010', '$100 million']},\n",
      " 'eBay': {'Craigslist': ['2004', '$13.5 million'],\n",
      "          'PayPal': ['2002', '$1500 million'],\n",
      "          'Skype Technologies': ['2005', '$2600 million']}}\n"
     ]
    }
   ],
   "source": [
    "pprint(acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAICS Code: Create company_naics_code dictionary.\n",
    "\n",
    "Note: Add company name to the company list in \"scrape_naics.py\".\n",
    "\n",
    "- Youtube ==> Youku \"can not find company\" ;\n",
    "\n",
    "- Paramount Pictures ==> Alibaba Pictures \"can not find company\" \n",
    "\n",
    "company_list = ['Alibaba', 'Walmart', 'eBay', 'Amazon','Youtube','Paramount Pictures','Costco','Kroger','Target','Vudu','Jet.com','Kmart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique naics codes.\n",
    "naics=dict() \n",
    "# Company and 6 digits Naics Code, which we need create relation between code and company. \n",
    "company_code=dict() \n",
    "\n",
    "with open('./naics_scraped_all.csv') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        company = row['Company']\n",
    "        code = row['NAICS_CODE']\n",
    "        title = row['NAICS_TITLE']\n",
    "        naics[code] = title\n",
    "        if company not in company_code.keys():\n",
    "             company_code[company]=code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alibaba': '561990',\n",
       " 'Walmart': '445110',\n",
       " 'eBay': '561990',\n",
       " 'Amazon': '443142',\n",
       " 'Paramount Pictures': '512191',\n",
       " 'Amblin': '334513',\n",
       " 'Vendio': '442110',\n",
       " 'Evening Post': '512191',\n",
       " 'Vudu': '518210',\n",
       " 'Jet.com': '237990',\n",
       " 'Shoebuy.com': '454110',\n",
       " 'Zappos': '448210',\n",
       " 'Whole Foods Market': '722310',\n",
       " 'Ring': '336412',\n",
       " 'Skype Technologies': '611310',\n",
       " 'PayPal': '551112',\n",
       " 'Craigslist': '541860'}"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change company names:\n",
    "## 'Paramount Pictures' -> 'Alibaba Pictures'\n",
    "## 'Ring' -> 'Ring Inc.' (Change to full name)\n",
    "## 'Amblin' -> 'Amblin Partners' (Change to full name)\n",
    "## 'Shoebuy.com' -> 'Shoes.com' (Shoebuy.com is former name)\n",
    "## 'Evening Post' -> 'South China Morning Post'\n",
    "\n",
    "company_code['Alibaba Pictures'] = company_code['Paramount Pictures']\n",
    "company_code['Ring Inc.'] = company_code['Ring']\n",
    "company_code['Amblin Partners'] = company_code['Amblin']\n",
    "company_code['Shoes.com'] = company_code['Shoebuy.com']\n",
    "company_code['South China Morning Post'] = company_code['Evening Post']\n",
    "\n",
    "del company_code['Paramount Pictures']\n",
    "del company_code['Ring']\n",
    "del company_code['Amblin']\n",
    "del company_code['Shoebuy.com']\n",
    "del company_code['Evening Post']\n",
    "\n",
    "# Add code for \"Ant Financial\"; use same code as PayPal\n",
    "company_code.update( {'Ant Financial' : company_code['PayPal'] } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique root code.\n",
    "root_code = [code for code in naics.keys() if len(code)==2 or re.match('\\d+-\\d+',code)]\n",
    "\n",
    "# Code with 6 digits.\n",
    "code6 = [code for code in naics.keys() if len(code)==6]\n",
    "# Code with 5 digits.\n",
    "code5 = [code for code in naics.keys() if len(code)==5 and not re.match('\\d+-\\d+',code)]\n",
    "# Code with 4 digits.\n",
    "code4 = [code for code in naics.keys() if len(code)==4]\n",
    "# Code with 3 digits.\n",
    "code3 = [code for code in naics.keys() if len(code)==3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['56', '44-45', '51', '31-33', '23', '72', '61', '55', '54']\n",
      "['561', '445', '443', '512', '334', '442', '518', '237', '454', '448', '722', '336', '611', '551', '541']\n",
      "['5619', '4451', '4431', '5121', '3345', '4421', '5182', '2379', '4541', '4482', '7223', '3364', '6113', '5511', '5418']\n",
      "['56199', '44511', '44314', '51219', '33451', '44211', '51821', '23799', '45411', '44821', '72231', '33641', '61131', '55111', '54186']\n",
      "['561990', '445110', '443142', '512191', '334513', '442110', '518210', '237990', '454110', '448210', '722310', '336412', '611310', '551112', '541860'] \n",
      "\n",
      "{'Alibaba': '561990',\n",
      " 'Alibaba Pictures': '512191',\n",
      " 'Amazon': '443142',\n",
      " 'Amblin Partners': '334513',\n",
      " 'Ant Financial': '551112',\n",
      " 'Craigslist': '541860',\n",
      " 'Jet.com': '237990',\n",
      " 'PayPal': '551112',\n",
      " 'Ring Inc.': '336412',\n",
      " 'Shoes.com': '454110',\n",
      " 'Skype Technologies': '611310',\n",
      " 'South China Morning Post': '512191',\n",
      " 'Vendio': '442110',\n",
      " 'Vudu': '518210',\n",
      " 'Walmart': '445110',\n",
      " 'Whole Foods Market': '722310',\n",
      " 'Zappos': '448210',\n",
      " 'eBay': '561990'}\n"
     ]
    }
   ],
   "source": [
    "print(root_code)\n",
    "print(code3)\n",
    "print(code4)\n",
    "print(code5)\n",
    "print(code6,'\\n')\n",
    "pprint(company_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph. \n",
    "### Company Nodes and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : I used the script in `'./scrapy_spider/scrapy_spider/spiders/wiki_spider_info.py'` to extracted the info box and saved in `info_box` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Company_Node(json):\n",
    "    \n",
    "    '''Create company node'''\n",
    "    revenue = get_val(json,'Revenue')\n",
    "    employee = get_val(json,'Number of employees')\n",
    "    if not employee:\n",
    "        employee = get_val(json, 'Employees')\n",
    "        if employee:\n",
    "            employee =employee.replace(\"c.\", '')\n",
    "    founded = get_val(json,'Founded')\n",
    "    revenue_num = None\n",
    "    employee_num = None\n",
    "    founded_yr = None\n",
    "    \n",
    "    if revenue:\n",
    "        revenue = revenue.lower()\n",
    "        r_has_billion = 'billion' in revenue\n",
    "        r_has_million = 'million' in revenue\n",
    "        \n",
    "        revenue_num = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", revenue)\n",
    "        revenue_num = float(re.sub(\"[^0-9\\.]\", \"\", revenue_num))\n",
    "        if r_has_billion:\n",
    "            revenue_num = revenue_num * 1000000000\n",
    "        elif r_has_million:\n",
    "            revenue_num = revenue_num * 1000000\n",
    "\n",
    "    if employee:\n",
    "        employee = employee.lower()\n",
    "        e_has_billion = 'billion' in employee\n",
    "        e_has_million = 'million' in employee\n",
    "        e_worldwide = 'worldwide' in employee\n",
    "        e_has_range = '–' in employee\n",
    "        if e_worldwide:\n",
    "            employee = employee.split('worldwide')[0]\n",
    "        if e_has_range:\n",
    "            employee = employee.split('–')[1]\n",
    "        employee_num = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", employee)\n",
    "        employee_num = float(re.sub(\"[^0-9\\.]\", \"\", employee_num))\n",
    "\n",
    "        if e_has_billion:\n",
    "            employee_num = employee_num * 1000000000\n",
    "        elif e_has_million:\n",
    "            employee_num = employee_num * 1000000\n",
    "    \n",
    "    if founded:\n",
    "        founded_yr = re.findall('(\\d{4})', founded)\n",
    "        if founded_yr:\n",
    "            founded_yr = int(founded_yr[0])\n",
    "\n",
    "    company_node = Node('Company', \n",
    "                    name=get_val(json,'Title'),\n",
    "                    organization_name=get_val(json,'Organization_name'),\n",
    "                    Type=get_val(json,'Type'),\n",
    "                    founded=get_val(json,'Founded'),\n",
    "                    founder=get_val(json,'Founder'),\n",
    "                    industry=get_val(json,'Industry'), \n",
    "                    products=get_val(json,'Products'), \n",
    "                    services=get_val(json,'Services'), \n",
    "                    revenue=get_val(json,'Revenue'), \n",
    "                    employees=get_val(json,'Number of employees'),\n",
    "                    parent=get_val(json,'Parent'),\n",
    "                    website=get_val(json,'Website'),\n",
    "                    employee_num=employee_num,\n",
    "                    revenue_num=revenue_num,\n",
    "                       founded_yr=founded_yr)\n",
    "    \n",
    "    company_node.__primarylabel__ = \"Company\"\n",
    "    company_node.__primarykey__ = \"name\"\n",
    "    \n",
    "    return company_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alibaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW) \n",
    "tx = g.begin()\n",
    "\n",
    "# Main company_Alibaba.\n",
    "with open('./info_box/Alibaba.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        Alibaba = Create_Company_Node(d)         \n",
    "        tx.create(Alibaba)\n",
    "\n",
    "# Competitor_Alibaba. \n",
    "# ** Please chage the Amazon title in Competitor_Alibaba.json': \"Amazon (Company)\" --> \"Amazon\" ** \n",
    "with open('./info_box/Alibaba_Competitor.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        company_node = Create_Company_Node(d)\n",
    "        relation = Relationship(Alibaba,'Competitor',company_node)\n",
    "        tx.create(relation)\n",
    "        \n",
    "# Acquisition_Alibaba.\n",
    "with open('./info_box/Alibaba_Acquisition.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        company_name = d['Title']\n",
    "        company_node = Create_Company_Node(d)\n",
    "        Y = acquisition['Alibaba'][company_name][0]\n",
    "        A = acquisition['Alibaba'][company_name][1]\n",
    "        relation = Relationship(Alibaba,'Acquired',company_node,year=Y,amount=A)\n",
    "        tx.create(relation)        \n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW) \n",
    "tx = g.begin()\n",
    "\n",
    "# Find Walmart node.\n",
    "matcher = NodeMatcher(g)\n",
    "Walmart = matcher.match('Company',name='Walmart').first()\n",
    "        \n",
    "# Walmart compete with Amazon and eBay as well.         \n",
    "Amazon = matcher.match('Company',name='Amazon').first()\n",
    "eBay = matcher.match('Company',name='eBay').first()\n",
    "tx.create(Relationship(Amazon,'Competitor',Walmart))\n",
    "tx.create(Relationship(eBay,'Competitor',Walmart))\n",
    "\n",
    "# Acquisition_Walmart. \n",
    "with open('./info_box/Walmart_Acquisition.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        company_name = d['Title']\n",
    "        company_node = Create_Company_Node(d)\n",
    "        Y = acquisition['Walmart'][company_name][0]\n",
    "        A = acquisition['Walmart'][company_name][1]\n",
    "        relation = Relationship(Walmart,'Acquired',company_node,year=Y,amount=A)\n",
    "        tx.create(relation)\n",
    "        \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW) \n",
    "tx = g.begin()\n",
    "\n",
    "# Find Walmart node.\n",
    "matcher = NodeMatcher(g)\n",
    "Amazon = matcher.match('Company',name='Amazon').first()\n",
    "        \n",
    "# Amazon Acquisitions. \n",
    "with open('./info_box/Amazon_Acquisition.json') as acquisitions:\n",
    "    data = json.load(acquisitions)\n",
    "    for d in data:\n",
    "        company_name = d['Title']\n",
    "        company_node = Create_Company_Node(d)\n",
    "        Y = acquisition['Amazon'][company_name][0]\n",
    "        A = acquisition['Amazon'][company_name][1]\n",
    "        relation = Relationship(Amazon,'Acquired',company_node,year=Y,amount=A)\n",
    "        tx.create(relation)\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eBay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Find Walmart node.\n",
    "matcher = NodeMatcher(g)\n",
    "eBay = matcher.match('Company',name='eBay').first()\n",
    "\n",
    "       \n",
    "# Amazon Acquisitions. \n",
    "with open('./info_box/eBay_Acquisition.json') as acquisitions:\n",
    "    data = json.load(acquisitions)\n",
    "    for d in data:\n",
    "        company_name = d['Title']\n",
    "        company_node = Create_Company_Node(d)\n",
    "        Y = acquisition['eBay'][company_name][0]\n",
    "        A = acquisition['eBay'][company_name][1]\n",
    "        relation = Relationship(eBay,'Acquired',company_node,year=Y,amount=A)\n",
    "        tx.create(relation)\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more relationships between companies.\n",
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "matcher = NodeMatcher(g)\n",
    "\n",
    "shoes = matcher.match('Company',name='Shoes.com').first()\n",
    "zappos = matcher.match('Company',name='Zappos').first()\n",
    "tx.create(Relationship(shoes,'Competitor',zappos))\n",
    "\n",
    "pp = matcher.match('Company',name='PayPal').first()\n",
    "af = matcher.match('Company',name='Ant Financial').first()\n",
    "tx.create(Relationship(pp,'Competitor',af))\n",
    "\n",
    "amazon = matcher.match('Company',name='Amazon').first()\n",
    "ebay = matcher.match('Company',name='eBay').first()\n",
    "jet = matcher.match('Company',name='Jet.com').first()\n",
    "tx.create(Relationship(jet,'Competitor',ebay))\n",
    "tx.create(Relationship(jet,'Competitor',amazon))\n",
    "\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAICS code nodes and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Code_Node(code):\n",
    "    \n",
    "    '''Create NAICS Ccode node'''\n",
    "    \n",
    "    code_node = Node('NAICS', code=code, title=naics[code])\n",
    "    \n",
    "    code_node.__primarylabel__ = \"NAICS\"\n",
    "    code_node.__primarykey__ = \"code\"\n",
    "    \n",
    "    return code_node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Code_Node(code):\n",
    "    \n",
    "    '''Check if NAICS Ccode node already exists'''\n",
    "    \n",
    "    node = matcher.match('NAICS', code=code)\n",
    "    \n",
    "    if node == None:\n",
    "        new = Create_Code_Node(code)\n",
    "        return new\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create Root Code Node. (2-digits or d-d)\n",
    "for root in root_code:\n",
    "    root_node = Create_Code_Node(root)\n",
    "    tx.create(root_node)\n",
    "    \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-digit-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create 3-digit-code Node.\n",
    "matcher = NodeMatcher(g)\n",
    "for code in code3:\n",
    "    node3 = Create_Code_Node(code)\n",
    "    \n",
    "    # Find the root code node.\n",
    "    if code[:-1] in ['44','45']:\n",
    "        node2 = matcher.match('NAICS', code='44-45').first()    \n",
    "    elif code[:-1] in ['31','33']:\n",
    "        node2 = matcher.match('NAICS', code='31-33').first()\n",
    "    else:\n",
    "        node2 = matcher.match('NAICS', code=code[:-1]).first()\n",
    "        \n",
    "    tx.create(Relationship(node3,'SubClassOf',node2))\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-digit-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create 4-digit-code Node.\n",
    "matcher = NodeMatcher(g)\n",
    "for code in code4:\n",
    "    node4 = Create_Code_Node(code)\n",
    "    node3 = matcher.match('NAICS', code=code[:-1]).first()\n",
    "    tx.create(Relationship(node4,'SubClassOf',node3))\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-digit-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create 5-digit-code Node.\n",
    "matcher = NodeMatcher(g)\n",
    "for code in code5:\n",
    "    node5 = Create_Code_Node(code)\n",
    "    node4 = matcher.match('NAICS', code=code[:-1]).first()\n",
    "    tx.create(Relationship(node5,'SubClassOf',node4))\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-digit-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create 5-digit-code Node.\n",
    "matcher = NodeMatcher(g)\n",
    "for code in code6:\n",
    "    node6 = Create_Code_Node(code)\n",
    "    node5 = matcher.match('NAICS', code=code[:-1]).first()\n",
    "    tx.create(Relationship(node6,'SubClassOf',node5))\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create relationships between company and 6-digit-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create relationships.\n",
    "matcher = NodeMatcher(g)\n",
    "for company in company_code:\n",
    "    CompanyNode = matcher.match('Company', name=company).first()\n",
    "    CodeNode = matcher.match('NAICS', code=company_code[company]).first()\n",
    "    tx.create(Relationship(CodeNode,'COMPANY',CompanyNode))\n",
    "    \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create controversies graph insertion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Controversy_Node(issue):\n",
    "    \n",
    "    '''Create controversy node'''\n",
    "    \n",
    "    node = Node('Controversy', issue=issue)\n",
    "    \n",
    "    node.__primarylabel__ = \"CONTROVERSY\"\n",
    "    node.__primarykey__ = \"controversy\"\n",
    "    \n",
    "    return node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Walmart': ['2010s crime problem'], 'Alibaba_Pictures': [], 'Vendio': [], 'Zappos': [], 'PayPal': ['150,000 PayPal cards frozen', 'CFPB consent'], 'Craigslist': [], 'Vudu': [], 'Jet.com': [], 'Shoes.com': [], 'Alibaba': ['Gold Supplier membership', 'Uranium sales', 'Counterfeit items and scams', 'Class action on IPO'], 'eBay': ['2014 security breach'], 'Amazon': ['Environmental impact', 'Selling counterfeit items', 'Sales and use taxes', 'Income taxes', 'Comments by Donald Trump and Bernie Sanders', 'Working conditions', 'Conflict of interest with the CIA and DOD', 'Seattle head tax and houselessness services', 'Nashville Operations Center of Excellence', 'Facial recognition technology and law enforcement'], 'Amblin Partners': [], 'South China Morning Post': ['Zhao Wei Incident', 'Closure of subsidiary publications', \"Criticism of Xi Jinping's ally withdrawn\", 'Publication of an interview made under duress'], 'Ant Financial': [], 'Ring Inc.': [], 'Whole Foods Market': ['Re-zoning petition regarding protected wetlands'], 'Skype Technologies': ['P2P licensing dispute lawsuit and IPO', 'China 2005', 'France 2005', 'United States, CALEA 2006', 'United States, Transparency and PRISM 2013']}\n"
     ]
    }
   ],
   "source": [
    "# table of contents particularly controversies\n",
    "controversy_dict = {}\n",
    "    \n",
    "with open('./table_of_content.txt') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    #mapping between data from toc and standarize\n",
    "    data['Alibaba'] = data.pop('Alibaba_Group')\n",
    "    data['eBay'] = data.pop('EBay')\n",
    "    data['Amazon'] = data.pop('Amazon_(company)')\n",
    "    data['Amblin Partners'] = data.pop('Amblin_Partners')\n",
    "    data['South China Morning Post'] = data.pop('South_China_Morning_Post')\n",
    "    data['Ant Financial'] = data.pop('Ant_Financial')\n",
    "    data['Ring Inc.'] = data.pop('Ring_Inc.')\n",
    "    data['Whole Foods Market'] = data.pop('Whole_Foods_Market')\n",
    "    data['Skype Technologies'] = data.pop('Skype_Technologies')\n",
    "    \n",
    "    \n",
    "    \n",
    "# company names we entered in neo4j:\n",
    "# Alibaba Pictures, Vendio, Amblin Partners\n",
    "# South China Morning Post, Ant Financial, Ring Inc., Whole Foods Market\n",
    "# Zappos, PayPal, Craigslist, Skype Technologies, Vudu, Jet.com, Shoes.com\n",
    "\n",
    "    for key,val in data.items():\n",
    "        controversy = []\n",
    "        for x in list(val.keys()):\n",
    "            if \"controve\" in x.lower() or \"criticism\" in x.lower() or \"issue\" in x.lower() \\\n",
    "                or \"fraud\" in x.lower() or \"litigation\" in x.lower():\n",
    "                controversy.append(x)\n",
    "        if len(controversy) > 0:\n",
    "            if (len(controversy)> 1):\n",
    "                cfields = []\n",
    "                for x in controversy:\n",
    "                    cfields.append(val[x])\n",
    "                controversy_dict[key] = [item for sublist in cfields for item in sublist] \n",
    "            else: \n",
    "                controversy_field = controversy[0]\n",
    "                controversy_dict[key] = val[controversy_field]\n",
    "        else:\n",
    "            controversy_dict[key] = []\n",
    "        \n",
    "\n",
    "    print(controversy_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Create relationships.\n",
    "matcher = NodeMatcher(g)\n",
    "for company, controversy_list in controversy_dict.items():\n",
    "    CompanyNode = matcher.match('Company', name=company).first()\n",
    "    for controversy in controversy_list:\n",
    "        controversy_node = Create_Controversy_Node(controversy)\n",
    "        tx.create(Relationship(CompanyNode,'HasControversy',controversy_node ))\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add in areas competing in from info box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Node_or_Create(name, primary_label, primary_key, **kwargs):\n",
    "    \n",
    "    '''Create  node'''\n",
    "    node = matcher.match(name, **kwargs).first()\n",
    "\n",
    "    if node == None:\n",
    "        node = Node(name, **kwargs )\n",
    "\n",
    "        node.__primarylabel__ = primary_label\n",
    "        node.__primarykey__ = primary_key\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "\n",
    "\n",
    "#as part of entity resolution, a simple way (no dedupe/nlp library) is to just have a mapping from company \n",
    "#to company as it is currently in neo4j \n",
    "company_mapping = {\n",
    "    \"Alibaba Group\": \"Alibaba\",\n",
    "    \"Amazon (company)\": \"Amazon\",\n",
    "    \"eBay\": \"eBay\",\n",
    "    \"Walmart\": \"Walmart\",\n",
    "    'EBay':'eBay',\n",
    "    'Alibaba Pictures': 'Alibaba Pictures',\n",
    "    'Amblin Partners': 'Amblin Partners',\n",
    "    'South China Morning Post': 'South China Morning Post',\n",
    "    'Ant Financial': 'Ant Financial',\n",
    "    'Ring Inc.': 'Ring Inc.',\n",
    "    'Whole Foods Market': 'Whole Foods Market',\n",
    "    'Skype Technologies': 'Skype Technologies',\n",
    "    'Vendio': 'Vendio',\n",
    "    'Zappos': 'Zappos', \n",
    "    \"PayPal\": \"PayPal\", \n",
    "    \"Craigslist\": \"Craigslist\", \n",
    "    \"Vudu\": \"Vudu\", \n",
    "    \"Jet.com\": \"Jet.com\", \n",
    "    \"Shoes.com\": \"Shoes.com\"\n",
    "}\n",
    "\n",
    "area_competing_in_map = {\n",
    "    \"Walmart\": \"Products\",\n",
    "    \"Amazon\": \"Industry\",\n",
    "    \"Alibaba\": \"Products\",\n",
    "    \"eBay\": \"Services\",\n",
    "    \"Zappos\": \"Industry\",\n",
    "    \"Whole Foods Market\": \"Industry\",\n",
    "    \"Vudu\": \"Services\",\n",
    "    \"Craigslist\": \"Services\",\n",
    "    \"Skype Technologies\": \"Industry\",\n",
    "    \"Jet.com\": \"Type of site\",\n",
    "    \"Shoes.com\": \"Industry\",\n",
    "    \"Alibaba Pictures\": \"Industry\",\n",
    "    \"Vendio\": \"Industry\",\n",
    "}\n",
    "with open('./info_box/all_info_box.json') as all_info_box:\n",
    "    info_box_data = json.load(all_info_box)\n",
    "\n",
    "    for company in info_box_data:\n",
    "        tx = g.begin()\n",
    "        company_title = company['Title'] if company['Title'] != None else company['Organization_name']\n",
    "        company_name = company_mapping[company_title]\n",
    "\n",
    "        area_competing_in_key = area_competing_in_map.get(company_name, None)\n",
    "        if (area_competing_in_key == None):\n",
    "            area_competing_in_key = \"Products\"\n",
    "\n",
    "        areas_competing_in = company.get(area_competing_in_key)\n",
    "        if (areas_competing_in):\n",
    "            CompanyNode = matcher.match('Company', name=company_name).first()\n",
    "            split_companies = areas_competing_in.split(\",\")\n",
    "            if len(split_companies) <2:\n",
    "                split_companies = areas_competing_in.split(\" \")\n",
    "            for area in split_companies:\n",
    "                node = Get_Node_or_Create('Product_Industry_Name', \"PRODUCT_INDUSTRY_NAME\", \"area_name\", product_industry_name=area.strip())\n",
    "                tx.create(Relationship(CompanyNode,'competes_in',node ))\n",
    "            tx.commit()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add Walmart, Jet.com, Vudu, Shoes.com nlp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Walmart\n",
    "\n",
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Add Walmart nlp.\n",
    "matcher = NodeMatcher(g)\n",
    "walmart=matcher.match('Company',name='Walmart').first()\n",
    "moosejaw=Node('Company',name='Moosejaw')\n",
    "r1=Relationship(walmart,'Acquired',moosejaw, year='2017', amount='$51 million')\n",
    "tx.create(r1)              \n",
    "\n",
    "#Add Attribute to Walmart.\n",
    "with open('./clean_nlp/clean_Walmart.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        if d['Edge']['Label']=='is':\n",
    "            Attribute=list(d['Node2'].keys())[0]\n",
    "            Property=list(d['Node2'].values())[0]\n",
    "            tx.merge(walmart)\n",
    "            walmart[Attribute] = Property\n",
    "            tx.push(walmart)\n",
    "\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jet.com\n",
    "\n",
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "# Add Jet.com nlp.\n",
    "matcher = NodeMatcher(g)\n",
    "jet=matcher.match('Company',name='Jet.com').first()\n",
    "amazon=matcher.match('Company',name='Amazon').first()\n",
    "person=Node('Person',name='Marc Lore')\n",
    "diapers=Node('Company',name='Diapers.com')\n",
    "r1=Relationship(person,'Sold',diapers,year='November,2010')\n",
    "tx.create(r1)\n",
    "r2=Relationship(person,'coFounded',jet)\n",
    "tx.create(r2)\n",
    "r3=Relationship(amazon,'Acquired',diapers,year='November,2010')\n",
    "tx.create(r3)\n",
    "\n",
    "# Add Attribute to Jet.com\n",
    "with open('./clean_nlp/clean_Jet.com.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        if d['Edge']['Label']=='is':\n",
    "            Attribute=list(d['Node2'].keys())[0]\n",
    "            Property=list(d['Node2'].values())[0]\n",
    "            tx.merge(jet)\n",
    "            jet[Attribute] = Property\n",
    "            tx.push(jet)\n",
    "               \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shoes.com\n",
    "\n",
    "# Connect database.\n",
    "graph = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = graph.begin()\n",
    "\n",
    "# Find Shoes.com node.\n",
    "matcher = NodeMatcher(g)\n",
    "shoe=matcher.match('Company',name='Shoes.com').first()\n",
    "\n",
    "# Add Attribute to Shoes.com\n",
    "with open('./clean_nlp/clean_Shoes.com.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        if d['Edge']['Label']=='is':\n",
    "            Attribute=list(d['Node2'].keys())[0]\n",
    "            Property=list(d['Node2'].values())[0]\n",
    "            tx.merge(shoe)\n",
    "            shoe[Attribute] = Property\n",
    "            tx.push(shoe)\n",
    "               \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vudu\n",
    "          \n",
    "# Connect database.\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "\n",
    "# Find Vudu node.\n",
    "matcher = NodeMatcher(g)\n",
    "vudu=matcher.match('Company',name='Vudu').first()\n",
    "\n",
    "# Add Attribute to Vudu.\n",
    "with open('./clean_nlp/clean_Vudu.json') as file:\n",
    "    data = json.load(file)\n",
    "    for d in data:\n",
    "        if d['Edge']['Label']=='is':\n",
    "            Attribute=list(d['Node2'].keys())[0]\n",
    "            Property=list(d['Node2'].values())[0]\n",
    "            tx.merge(vudu)\n",
    "            vudu[Attribute] = Property\n",
    "            tx.push(vudu)\n",
    "               \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Adding Alibaba and affiliate (Amblin Partners, Ant Financial, South China Morning Post)\n",
    "#### Uses 3 separate csvs to model nodes, node attributes, and relationships between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes \n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "with open('./clean_nlp/node_labels.csv','rt')as f:\n",
    "    node_labels = csv.reader(f)\n",
    "    for row in node_labels:\n",
    "        \n",
    "        name = row[0]\n",
    "        node_type = row[1]\n",
    "        \n",
    "        node = Node(node_type, name=name)\n",
    "        node.__primarylabel__ = node_type\n",
    "        node.__primarykey__ = \"name\"\n",
    "        \n",
    "        tx.create(node)\n",
    "        \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node attributes\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "matcher = NodeMatcher(g)\n",
    "\n",
    "with open('./clean_nlp/node_property.csv','rt')as f:\n",
    "    node_property = csv.reader(f)\n",
    "    for row in node_property:\n",
    "        \n",
    "        node = matcher.match(row[0],name=row[1]).first()\n",
    "        \n",
    "        key = row[2]\n",
    "        value = row[3]\n",
    "        \n",
    "        # ignore nodes that have already have the specific key within them\n",
    "        if key in node:\n",
    "            continue\n",
    "        \n",
    "        tx.merge(node)\n",
    "        node[key] = value\n",
    "        tx.push(node)\n",
    "                \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node relationships\n",
    "g = Graph(host=HOST,user=USER,password=PW)\n",
    "tx = g.begin()\n",
    "\n",
    "matcher = NodeMatcher(g)\n",
    "\n",
    "with open('./clean_nlp/node_connectivity.json', 'r') as f:\n",
    "    node_c = json.load(f)\n",
    "        \n",
    "for j in node_c:\n",
    "    \n",
    "    for k in j['Node1'].keys():\n",
    "        label1 = k\n",
    "    for k in j['Node2'].keys():\n",
    "        label2 = k\n",
    "        \n",
    "    node1 = matcher.match(label1,name=j['Node1'][label1]).first()\n",
    "    node2 = matcher.match(label2,name=j['Node2'][label2]).first()\n",
    "    \n",
    "    r = Relationship(node1, j['Edge']['Label'], node2)\n",
    "    for k in j['Edge']:\n",
    "        if k == 'Label':\n",
    "            continue\n",
    "        else:\n",
    "            r[k] = j['Edge'][k]\n",
    "            \n",
    "    tx.create(r)\n",
    "\n",
    "tx.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
