{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of urls\n",
    "raw_d = {}\n",
    "urls = ['https://en.wikipedia.org/wiki/Alibaba_Group']\n",
    "\n",
    "# layout of wiki page is standardized\n",
    "# text content is all in p tags\n",
    "for u in urls:\n",
    "    \n",
    "    # dictionary to to hold text information by url\n",
    "    # in case we try and crawl all on a central notebook\n",
    "    raw_d[u] = []\n",
    "    wiki_pg = requests.get(u)\n",
    "    soup = BS(wiki_pg.content, 'lxml')\n",
    "    for content in soup.select(\"p\"):\n",
    "        # adding to list\n",
    "        raw_d[u].append(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0xae24786a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "import textacy\n",
    "import neuralcoref\n",
    "\n",
    "# will need to download eng trained model via cli\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "# there are 3 types of basic trained models (sm, md, lg)\n",
    "# using medium as the neural coref came out funky with large\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_d = {}\n",
    "\n",
    "for u in urls:\n",
    "    resolved_p = []\n",
    "    \n",
    "    full_text = raw_d[u]\n",
    "    \n",
    "    for paragraph in full_text:\n",
    "        \n",
    "        p = paragraph.strip()\n",
    "        doc = nlp(p)\n",
    "        \n",
    "        # apply neural coref to resolve word references\n",
    "        cluster = doc._.coref_clusters\n",
    "        resolved_p.append(doc._.coref_resolved)\n",
    "        \n",
    "    resolved_d[u] = resolved_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that will contain relationships from all methods\n",
    "# key = url \n",
    "# value = list of json of relationship described in each method\n",
    "\n",
    "total_d = {}\n",
    "for u in resolved_d.keys():\n",
    "    total_d[u] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO LEMMA THE SENTENCES\n",
    "# SOME SENTENCES ARE BETTER LEMMA-ED OTHERS ARE NOT\n",
    "\n",
    "LEMMA = False\n",
    "\n",
    "if LEMMA:\n",
    "    for u in resolved_d.keys():\n",
    "\n",
    "        corpus = resolved_d[u]\n",
    "        lemma_corpus = []\n",
    "\n",
    "        for p in corpus:\n",
    "            doc = nlp(p)\n",
    "\n",
    "            lemma_doc = []\n",
    "            for token in doc:\n",
    "                # keeping is\n",
    "                if token.pos == VERB:\n",
    "                    if token.lemma_ == 'be':\n",
    "                        lemma_doc.append('is')\n",
    "                    else:\n",
    "                        lemma_doc.append(token.lemma_)\n",
    "                else:\n",
    "                    lemma_doc.append(token.text)\n",
    "\n",
    "            lemma_doc = ' '.join(lemma_doc).strip()\n",
    "            lemma_corpus.append(lemma_doc)\n",
    "\n",
    "        resolved_d[u] = lemma_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "\n",
    "### Trys to extract nouns and verbs with preposition phrases based on the token dependency tree from pos tagging ###\n",
    "# Searches dependency tree and if a verb is found, will then traverse the tree left and right\n",
    "# to determine the relationship in the sentence\n",
    "# will also try and find the preopsitional phrase in the sentence if any and try to add that to the relationship as well\n",
    "\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "# prep = description of edge\n",
    "\n",
    "def get_preps(doc):\n",
    "    preps = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADP':\n",
    "            pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            preps.append(pp)\n",
    "    return preps\n",
    "\n",
    "def get_noun_verb(doc):\n",
    "    \n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            \n",
    "            subject = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            after = [w for w in token.head.rights]\n",
    "            \n",
    "            if len(subject) == 0:\n",
    "                subject = [token.head]\n",
    "                after = [w for w in token.rights]\n",
    "            \n",
    "            relation_dict = {\n",
    "                'before': [],\n",
    "                'after': [],\n",
    "                'verb': token,\n",
    "                'prep': []\n",
    "            }\n",
    "            \n",
    "            relation_dict['before'].extend(subject)\n",
    "            relation_dict['after'].extend(after)\n",
    "            \n",
    "            relations.append(relation_dict)\n",
    "            \n",
    "    return relations\n",
    "\n",
    "def combine_noun_verb_preps(noun_verbs, preps):\n",
    "    holder = []\n",
    "    \n",
    "    for nv in noun_verbs:\n",
    "        \n",
    "        # will apply before preposition and after preposition\n",
    "        holder.append(nv)\n",
    "        nv['prep'].extend(preps)\n",
    "        holder.append(nv)\n",
    "    \n",
    "    return holder\n",
    "\n",
    "KEY = 'Method1'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method1\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            noun_verbs = get_noun_verb(doc)\n",
    "            preps = get_preps(doc)\n",
    "            combined_nvp = combine_noun_verb_preps(noun_verbs, preps)\n",
    "            \n",
    "            total_d[u][KEY].append(combined_nvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "### Similar to method 1 ###\n",
    "# Extracts noun chunks and verbs and creates span of nouns and verbs \n",
    "# spans are nouns before a verb, verb, and then nouns after\n",
    "# For example sentence may be: nouns1 + verb1 + nouns2 + verb2 + nouns3 \n",
    "# the spans will be:\n",
    "# - nouns1, verb1, nouns2\n",
    "# - nouns2, verb2, nouns3\n",
    "\n",
    "# theyre all placed in json and is in a similar style to method 1\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "\n",
    "def get_nouns(doc):\n",
    "    nouns = []\n",
    "    noun = textacy.extract.noun_chunks(doc)\n",
    "\n",
    "    for n in noun:\n",
    "\n",
    "        end = n.end\n",
    "        n = str(n)\n",
    "        temp_word = []\n",
    "        for ch in n:\n",
    "            if ch.isalpha() or ch.isnumeric() or ch is ' ':\n",
    "                temp_word.append(ch)\n",
    "\n",
    "        nouns.append((''.join(temp_word), end))\n",
    "        \n",
    "    return nouns\n",
    "\n",
    "def get_verbs(doc):\n",
    "    main_verbs = textacy.spacier.utils.get_main_verbs_of_sent(doc)\n",
    "    verbs = []\n",
    "    for verb in main_verbs:\n",
    "\n",
    "        verb_index = verb.i\n",
    "        verbs.append((verb.text, verb_index))\n",
    "\n",
    "    tbd = []\n",
    "    for i in range(len(verbs)):\n",
    "        v = verbs[i]\n",
    "\n",
    "        for n in nouns:\n",
    "            if v[0] in n[0]:\n",
    "                tbd.append(v)\n",
    "\n",
    "    cleaned_verbs = []\n",
    "    for v in verbs:\n",
    "        if v in tbd:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_verbs.append(v)\n",
    "\n",
    "    return cleaned_verbs\n",
    "\n",
    "def span_relation(nouns, verbs):\n",
    "    relations = []\n",
    "\n",
    "    # ensure we can collect appropriate spans by setting bounds\n",
    "    before = 0\n",
    "    end = float('inf')\n",
    "\n",
    "    for i in range(len(verbs)):\n",
    "\n",
    "        v = verbs[i]\n",
    "\n",
    "        # helper variables to handle span creation\n",
    "        if i == 0:\n",
    "            before = float('-inf')\n",
    "        else:\n",
    "            before = i - 1\n",
    "\n",
    "        if i == len(verbs) - 1:\n",
    "            after = float('inf')\n",
    "        else:\n",
    "            after = i + 1\n",
    "\n",
    "        relation_dict = {\n",
    "            'before': [],\n",
    "            'verb': v[0],\n",
    "            'after': []\n",
    "        }\n",
    "\n",
    "        # creating spans of noun chunks and verbs\n",
    "        for n in nouns:\n",
    "\n",
    "            if before == float('-inf'):\n",
    "                if n[1] <= verbs[0][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "            else:\n",
    "                if n[1] < v[1] and n[1] > verbs[before][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "                elif n[1] == v[1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "\n",
    "        for n in nouns:\n",
    "\n",
    "            if after == float('inf'):\n",
    "                if n[1] > verbs[-1][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "            else:\n",
    "                if n[1] > v[1] and n[1] <= verbs[after][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "\n",
    "        relations.append(relation_dict)\n",
    "        \n",
    "    return relations\n",
    "\n",
    "KEY = 'Method2'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method2\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            nouns = get_nouns(doc)\n",
    "            verbs = get_verbs(doc)\n",
    "            sr = span_relation(nouns, verbs)\n",
    "            \n",
    "            total_d[u][KEY].append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
