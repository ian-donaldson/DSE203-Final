{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of urls\n",
    "raw_d = {}\n",
    "\n",
    "# Alibaba\n",
    "# acquisitions: Amblin Partners, Ant Financial, South China Morning Post\n",
    "\n",
    "# replace urls list with your own list of urls \n",
    "urls = ['https://en.wikipedia.org/wiki/Alibaba_Group', 'https://en.wikipedia.org/wiki/Amblin_Partners',\n",
    "       'https://en.wikipedia.org/wiki/Ant_Financial', 'https://en.wikipedia.org/wiki/South_China_Morning_Post']\n",
    "\n",
    "# urls = ['https://en.wikipedia.org/wiki/Alibaba_Group']\n",
    "\n",
    "# layout of wiki page is standardized\n",
    "# text content is all in p tags\n",
    "for u in urls:\n",
    "    \n",
    "    # dictionary to to hold text information by url\n",
    "    # in case we try and crawl all on a central notebook\n",
    "    raw_d[u] = []\n",
    "    wiki_pg = requests.get(u)\n",
    "    soup = BS(wiki_pg.content, 'lxml')\n",
    "    for content in soup.select(\"p\"):\n",
    "        # adding to list\n",
    "        raw_d[u].append(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0xa39267ef0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "import textacy\n",
    "import neuralcoref\n",
    "\n",
    "# will need to download eng trained model via cli\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "# there are 3 types of basic trained models (sm, md, lg)\n",
    "# using medium as the neural coref came out funky with large\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_d = {}\n",
    "\n",
    "for u in urls:\n",
    "    resolved_p = []\n",
    "    \n",
    "    full_text = raw_d[u]\n",
    "    \n",
    "    for paragraph in full_text:\n",
    "        \n",
    "        p = paragraph.strip()\n",
    "        doc = nlp(p)\n",
    "        \n",
    "        # apply neural coref to resolve word references\n",
    "        cluster = doc._.coref_clusters\n",
    "        resolved_p.append(doc._.coref_resolved)\n",
    "        \n",
    "    resolved_d[u] = resolved_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that will contain relationships from all methods\n",
    "# key = url \n",
    "# value = list of json of relationship described in each method\n",
    "\n",
    "total_d = {}\n",
    "for u in resolved_d.keys():\n",
    "    total_d[u] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO LEMMA THE SENTENCES\n",
    "# SOME SENTENCES ARE BETTER LEMMA-ED OTHERS ARE NOT\n",
    "\n",
    "LEMMA = False\n",
    "\n",
    "if LEMMA:\n",
    "    for u in resolved_d.keys():\n",
    "\n",
    "        corpus = resolved_d[u]\n",
    "        lemma_corpus = []\n",
    "\n",
    "        for p in corpus:\n",
    "            doc = nlp(p)\n",
    "\n",
    "            lemma_doc = []\n",
    "            for token in doc:\n",
    "                # keeping is\n",
    "                if token.pos == VERB:\n",
    "                    if token.lemma_ == 'be':\n",
    "                        lemma_doc.append('is')\n",
    "                    else:\n",
    "                        lemma_doc.append(token.lemma_)\n",
    "                else:\n",
    "                    lemma_doc.append(token.text)\n",
    "\n",
    "            lemma_doc = ' '.join(lemma_doc).strip()\n",
    "            lemma_corpus.append(lemma_doc)\n",
    "\n",
    "        resolved_d[u] = lemma_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "\n",
    "### Trys to extract nouns and verbs with preposition phrases based on the token dependency tree from pos tagging ###\n",
    "# Searches dependency tree and if a verb is found, will then traverse the tree left and right\n",
    "# to determine the relationship in the sentence\n",
    "# will also try and find the preopsitional phrase in the sentence if any and try to add that to the relationship as well\n",
    "\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "# prep = description of edge\n",
    "\n",
    "def get_preps(doc):\n",
    "    preps = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADP':\n",
    "            pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            preps.append(pp)\n",
    "    return preps\n",
    "\n",
    "def get_noun_verb(doc):\n",
    "    \n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            \n",
    "            subject = [w.text for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            after = [w.text for w in token.head.rights]\n",
    "            \n",
    "            if len(subject) == 0:\n",
    "                subject = [token.head.text]\n",
    "                after = [w.text for w in token.rights]\n",
    "            \n",
    "            relation_dict = {\n",
    "                'before': [],\n",
    "                'after': [],\n",
    "                'verb': token.text,\n",
    "                'prep': []\n",
    "            }\n",
    "            \n",
    "            relation_dict['before'].extend(subject)\n",
    "            relation_dict['after'].extend(after)\n",
    "            \n",
    "            relations.append(relation_dict)\n",
    "            \n",
    "    return relations\n",
    "\n",
    "def combine_noun_verb_preps(noun_verbs, preps):\n",
    "    holder = []\n",
    "    \n",
    "    for nv in noun_verbs:\n",
    "        \n",
    "        # will apply before preposition and after preposition\n",
    "        holder.append(nv)\n",
    "        nv['prep'].extend(preps)\n",
    "        holder.append(nv)\n",
    "    \n",
    "    return holder\n",
    "\n",
    "KEY = 'Method1'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method1\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            noun_verbs = get_noun_verb(doc)\n",
    "            preps = get_preps(doc)\n",
    "            combined_nvp = combine_noun_verb_preps(noun_verbs, preps)\n",
    "            \n",
    "            total_d[u][KEY].append(combined_nvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "### Similar to method 1 ###\n",
    "# Extracts noun chunks and verbs and creates span of nouns and verbs \n",
    "# spans are nouns before a verb, verb, and then nouns after\n",
    "# For example sentence may be: nouns1 + verb1 + nouns2 + verb2 + nouns3 \n",
    "# the spans will be:\n",
    "# - nouns1, verb1, nouns2\n",
    "# - nouns2, verb2, nouns3\n",
    "\n",
    "# theyre all placed in json and is in a similar style to method 1\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "\n",
    "def get_nouns(doc):\n",
    "    nouns = []\n",
    "    noun = textacy.extract.noun_chunks(doc)\n",
    "\n",
    "    for n in noun:\n",
    "\n",
    "        end = n.end\n",
    "        n = str(n)\n",
    "        temp_word = []\n",
    "        for ch in n:\n",
    "            if ch.isalpha() or ch.isnumeric() or ch is ' ':\n",
    "                temp_word.append(ch)\n",
    "\n",
    "        nouns.append((''.join(temp_word), end))\n",
    "        \n",
    "    return nouns\n",
    "\n",
    "def get_verbs(doc):\n",
    "    main_verbs = textacy.spacier.utils.get_main_verbs_of_sent(doc)\n",
    "    verbs = []\n",
    "    for verb in main_verbs:\n",
    "\n",
    "        verb_index = verb.i\n",
    "        verbs.append((verb.text, verb_index))\n",
    "\n",
    "    tbd = []\n",
    "    for i in range(len(verbs)):\n",
    "        v = verbs[i]\n",
    "\n",
    "        for n in nouns:\n",
    "            if v[0] in n[0]:\n",
    "                tbd.append(v)\n",
    "\n",
    "    cleaned_verbs = []\n",
    "    for v in verbs:\n",
    "        if v in tbd:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_verbs.append(v)\n",
    "\n",
    "    return cleaned_verbs\n",
    "\n",
    "def span_relation(nouns, verbs):\n",
    "    relations = []\n",
    "\n",
    "    # ensure we can collect appropriate spans by setting bounds\n",
    "    before = 0\n",
    "    end = float('inf')\n",
    "\n",
    "    for i in range(len(verbs)):\n",
    "\n",
    "        v = verbs[i]\n",
    "\n",
    "        # helper variables to handle span creation\n",
    "        if i == 0:\n",
    "            before = float('-inf')\n",
    "        else:\n",
    "            before = i - 1\n",
    "\n",
    "        if i == len(verbs) - 1:\n",
    "            after = float('inf')\n",
    "        else:\n",
    "            after = i + 1\n",
    "\n",
    "        relation_dict = {\n",
    "            'before': [],\n",
    "            'verb': v[0],\n",
    "            'after': []\n",
    "        }\n",
    "\n",
    "        # creating spans of noun chunks and verbs\n",
    "        for n in nouns:\n",
    "\n",
    "            if before == float('-inf'):\n",
    "                if n[1] <= verbs[0][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "            else:\n",
    "                if n[1] < v[1] and n[1] > verbs[before][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "                elif n[1] == v[1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "\n",
    "        for n in nouns:\n",
    "\n",
    "            if after == float('inf'):\n",
    "                if n[1] > verbs[-1][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "            else:\n",
    "                if n[1] > v[1] and n[1] <= verbs[after][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "\n",
    "        relations.append(relation_dict)\n",
    "        \n",
    "    return relations\n",
    "\n",
    "KEY = 'Method2'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method2\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            nouns = get_nouns(doc)\n",
    "            verbs = get_verbs(doc)\n",
    "            sr = span_relation(nouns, verbs)\n",
    "            \n",
    "            total_d[u][KEY].append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Method\n",
    "\n",
    "# speciify cues (currently set as 'be' or 'is')\n",
    "# and will grab those relationships\n",
    "\n",
    "KEY = 'Attributes'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # using the raw form\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                # cue - currently set as default ('be')\n",
    "                # can be modified for different verbs\n",
    "                relationship = textacy.extract.semistructured_statements(doc, ent.text)\n",
    "                for r in relationship:\n",
    "                    \n",
    "                    l = []\n",
    "                    for words in r:\n",
    "                        l.append(str(words))\n",
    "                        \n",
    "                    total_d[u][KEY].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money\n",
    "\n",
    "# taken from spacy website\n",
    "# added the rest of the spans to the beginning of the relation as well\n",
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result\n",
    "\n",
    "def extract_currency_relations(doc):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "    \n",
    "    str_noun_chunks = []\n",
    "    for n in list(doc.noun_chunks):\n",
    "        str_noun_chunks.append(str(n))\n",
    "        \n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w.text for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                # subject = subject[0]\n",
    "                relations.append((subject, money.text, list(str_noun_chunks)))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head.text, money.text))\n",
    "    return relations\n",
    "\n",
    "\n",
    "KEY = 'Money'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # using the raw form\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            money_data = extract_currency_relations(doc)\n",
    "            if len(money_data) > 0:\n",
    "                total_d[u][KEY].append(money_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick output of everything\n",
    "import json\n",
    "\n",
    "\n",
    "for k in total_d.keys():\n",
    "    \n",
    "    filename = '{0}.json'.format(k)\n",
    "    fil = 'raw_' + filename.split('/')[-1]\n",
    "    \n",
    "    with open(fil, 'w') as outfile:\n",
    "        json.dump(total_d[k], outfile, sort_keys=True, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
