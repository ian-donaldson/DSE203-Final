{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of urls\n",
    "raw_d = {}\n",
    "\n",
    "# Alibaba\n",
    "# acquisitions: Amblin Partners, Ant Financial, South China Morning Post\n",
    "\n",
    "# replace urls list with your own list of urls \n",
    "urls = ['https://en.wikipedia.org/wiki/Alibaba_Group', 'https://en.wikipedia.org/wiki/Amblin_Partners',\n",
    "       'https://en.wikipedia.org/wiki/Ant_Financial', 'https://en.wikipedia.org/wiki/South_China_Morning_Post']\n",
    "\n",
    "urls = ['https://en.wikipedia.org/wiki/Alibaba_Group']\n",
    "\n",
    "# layout of wiki page is standardized\n",
    "# text content is all in p tags\n",
    "for u in urls:\n",
    "    \n",
    "    # dictionary to to hold text information by url\n",
    "    # in case we try and crawl all on a central notebook\n",
    "    raw_d[u] = []\n",
    "    wiki_pg = requests.get(u)\n",
    "    soup = BS(wiki_pg.content, 'lxml')\n",
    "    for content in soup.select(\"p\"):\n",
    "        # adding to list\n",
    "        raw_d[u].append(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0xa2a77ab00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "import textacy\n",
    "import neuralcoref\n",
    "\n",
    "# will need to download eng trained model via cli\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "# there are 3 types of basic trained models (sm, md, lg)\n",
    "# using medium as the neural coref came out funky with large\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_d = {}\n",
    "\n",
    "for u in urls:\n",
    "    resolved_p = []\n",
    "    \n",
    "    full_text = raw_d[u]\n",
    "    \n",
    "    for paragraph in full_text:\n",
    "        \n",
    "        p = paragraph.strip()\n",
    "        doc = nlp(p)\n",
    "        \n",
    "        # apply neural coref to resolve word references\n",
    "        cluster = doc._.coref_clusters\n",
    "        resolved_p.append(doc._.coref_resolved)\n",
    "        \n",
    "    resolved_d[u] = resolved_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that will contain relationships from all methods\n",
    "# key = url \n",
    "# value = list of json of relationship described in each method\n",
    "\n",
    "total_d = {}\n",
    "for u in resolved_d.keys():\n",
    "    total_d[u] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO LEMMA THE SENTENCES\n",
    "# SOME SENTENCES ARE BETTER LEMMA-ED OTHERS ARE NOT\n",
    "\n",
    "LEMMA = True\n",
    "\n",
    "if LEMMA:\n",
    "    for u in resolved_d.keys():\n",
    "\n",
    "        corpus = resolved_d[u]\n",
    "        lemma_corpus = []\n",
    "\n",
    "        for p in corpus:\n",
    "            doc = nlp(p)\n",
    "\n",
    "            lemma_doc = []\n",
    "            for token in doc:\n",
    "                # keeping is\n",
    "                if token.pos == VERB:\n",
    "                    if token.lemma_ == 'be':\n",
    "                        lemma_doc.append('is')\n",
    "                    else:\n",
    "                        lemma_doc.append(token.lemma_)\n",
    "                else:\n",
    "                    lemma_doc.append(token.text)\n",
    "\n",
    "            lemma_doc = ' '.join(lemma_doc).strip()\n",
    "            lemma_corpus.append(lemma_doc)\n",
    "\n",
    "        resolved_d[u] = lemma_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba Group Holding Limited, (also known as Alibaba Group and as Alibaba), is a Chinese multinational conglomerate holding company specializing in e-commerce, retail, Internet, and technology. Founded on 4 April 1999 in Hangzhou, Zhejiang, the company provides consumer-to-consumer (C2C), business-to-consumer (B2C), and business-to-business (B2B) sales services via web portals, as well as electronic payment services, shopping search engines and cloud computing services. It owns and operates a diverse array of businesses around the world in numerous sectors, and is named as one of the world's most admired companies by Fortune.[3][4]\n",
      "\n",
      "Alibaba Group Holding Limited , ( also know as Alibaba Group and as Alibaba ) , is a Chinese multinational conglomerate hold company specialize in e - commerce , retail , Internet , and technology . found on 4 April 1999 in Hangzhou , Zhejiang , Alibaba provide consumer - to - consumer ( C2C ) , business - to - consumer ( B2C ) , and business - to - business ( B2B ) sales services via web portals , as well as electronic payment services , shopping search engines and cloud computing services . Alibaba own and operate a diverse array of businesses around the world in numerous sectors , and is name as one of the world 's most admired companies by Fortune.[3][4 ]\n"
     ]
    }
   ],
   "source": [
    "key = 'https://en.wikipedia.org/wiki/Alibaba_Group'\n",
    "\n",
    "print (raw_d[key][1])\n",
    "print (resolved_d[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'verb': 'know', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['as', ')']}, {'verb': 'know', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['as', ')']}, {'verb': 'is', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['hold']}, {'verb': 'is', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['hold']}, {'verb': 'hold', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['specialize']}, {'verb': 'hold', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['is'], 'after': ['specialize']}, {'verb': 'specialize', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['a Chinese multinational conglomerate'], 'after': ['specialize']}, {'verb': 'specialize', 'prep': ['as Alibaba Group and as Alibaba', 'as Alibaba', 'in e - commerce , retail , Internet , and technology'], 'before': ['a Chinese multinational conglomerate'], 'after': ['specialize']}]\n",
      "[{'verb': 'is', 'prep': [\"At closing time on the date of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", \"on the date of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", \"of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", 'in history'], 'before': [\"Alibaba 's market value\"], 'after': ['US$ 231 billion']}, {'verb': 'is', 'prep': [\"At closing time on the date of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", \"on the date of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", \"of its initial public offering ( IPO ) – US$ 25 billion – the world 's highest in history , 19 September 2014\", 'in history'], 'before': [\"Alibaba 's market value\"], 'after': ['US$ 231 billion']}]\n",
      "[{'verb': 'is', 'prep': ['of the largest Internet and artificial intelligence companies', 'of the biggest venture capital firms', 'of the biggest investment corporations in the world', 'in the world'], 'before': ['Alibaba'], 'after': [\"the world 's largest retailer and e - commerce company\"]}, {'verb': 'is', 'prep': ['of the largest Internet and artificial intelligence companies', 'of the biggest venture capital firms', 'of the biggest investment corporations in the world', 'in the world'], 'before': ['Alibaba'], 'after': [\"the world 's largest retailer and e - commerce company\"]}]\n",
      "[{'verb': 'come', 'prep': ['from the character Ali Baba', 'from the Middle Eastern folk tale collection One Thousand and One Nights', \"because of The company 's name universal appeal\", 'of'], 'before': [\"The company 's name\"], 'after': ['from', 'from', 'because']}, {'verb': 'come', 'prep': ['from the character Ali Baba', 'from the Middle Eastern folk tale collection One Thousand and One Nights', \"because of The company 's name universal appeal\", 'of'], 'before': [\"The company 's name\"], 'after': ['from', 'from', 'because']}]\n",
      "[{'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['I'], 'after': ['in', 'in', ',', 'and', 'think']}, {'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['I'], 'after': ['in', 'in', ',', 'and', 'think']}, {'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['think'], 'after': []}, {'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['think'], 'after': []}, {'verb': 'think', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['I'], 'after': ['in', 'in', ',', 'and', 'think']}, {'verb': 'think', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['I'], 'after': ['in', 'in', ',', 'and', 'think']}, {'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['think'], 'after': ['a good name']}, {'verb': 'is', 'prep': ['in San Francisco', 'in a coffee shop'], 'before': ['think'], 'after': ['a good name']}]\n",
      "[{'verb': 'found', 'prep': ['On 4 April 1999', 'of 17 friends and'], 'before': ['Jack Ma and Jack Ma team'], 'after': ['Alibaba']}, {'verb': 'found', 'prep': ['On 4 April 1999', 'of 17 friends and'], 'before': ['Jack Ma and Jack Ma team'], 'after': ['Alibaba']}]\n",
      "[{'verb': 'announce', 'prep': ['into China', 'in 2003', 'as a foreign competitor', 'of Alibaba subsidiary Taobao'], 'before': ['Ma'], 'after': ['eBay', 'as', 'and', 'reject']}, {'verb': 'announce', 'prep': ['into China', 'in 2003', 'as a foreign competitor', 'of Alibaba subsidiary Taobao'], 'before': ['Ma'], 'after': ['eBay', 'as', 'and', 'reject']}, {'verb': 'reject', 'prep': ['into China', 'in 2003', 'as a foreign competitor', 'of Alibaba subsidiary Taobao'], 'before': ['Ma'], 'after': ['eBay', 'as', 'and', 'reject']}, {'verb': 'reject', 'prep': ['into China', 'in 2003', 'as a foreign competitor', 'of Alibaba subsidiary Taobao'], 'before': ['Ma'], 'after': ['eBay', 'as', 'and', 'reject']}]\n",
      "[{'verb': 'invest', 'prep': ['In 2005', 'in Alibaba', 'through a variable interest entity ( VIE ) structure', 'in Yahoo', 'for US$ 1 billion'], 'before': ['Yahoo'], 'after': ['in', 'through', ',', 'buy']}, {'verb': 'invest', 'prep': ['In 2005', 'in Alibaba', 'through a variable interest entity ( VIE ) structure', 'in Yahoo', 'for US$ 1 billion'], 'before': ['Yahoo'], 'after': ['in', 'through', ',', 'buy']}, {'verb': 'buy', 'prep': ['In 2005', 'in Alibaba', 'through a variable interest entity ( VIE ) structure', 'in Yahoo', 'for US$ 1 billion'], 'before': ['Yahoo'], 'after': ['in', 'through', ',', 'buy']}, {'verb': 'buy', 'prep': ['In 2005', 'in Alibaba', 'through a variable interest entity ( VIE ) structure', 'in Yahoo', 'for US$ 1 billion'], 'before': ['Yahoo'], 'after': ['in', 'through', ',', 'buy']}]\n",
      "[{'verb': 'is', 'prep': ['to Li Chuan , a senior executive at Alibaba', 'at Alibaba', 'in 2013', 'in partnership with Chinese real estate company Wanda Group', 'with Chinese real estate company Wanda Group'], 'before': ['Alibaba'], 'after': ['plan', 'open']}, {'verb': 'is', 'prep': ['to Li Chuan , a senior executive at Alibaba', 'at Alibaba', 'in 2013', 'in partnership with Chinese real estate company Wanda Group', 'with Chinese real estate company Wanda Group'], 'before': ['Alibaba'], 'after': ['plan', 'open']}, {'verb': 'open', 'prep': ['to Li Chuan , a senior executive at Alibaba', 'at Alibaba', 'in 2013', 'in partnership with Chinese real estate company Wanda Group', 'with Chinese real estate company Wanda Group'], 'before': ['Alibaba'], 'after': ['plan', 'open']}, {'verb': 'open', 'prep': ['to Li Chuan , a senior executive at Alibaba', 'at Alibaba', 'in 2013', 'in partnership with Chinese real estate company Wanda Group', 'with Chinese real estate company Wanda Group'], 'before': ['Alibaba'], 'after': ['plan', 'open']}]\n",
      "[{'verb': 'lead', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['Alibaba'], 'after': ['a US$ 250 million Series D financing round', ',', 'bring']}, {'verb': 'lead', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['Alibaba'], 'after': ['a US$ 250 million Series D financing round', ',', 'bring']}, {'verb': 'is', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['a US$ 250 million Series D financing round'], 'after': ['complete']}, {'verb': 'is', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['a US$ 250 million Series D financing round'], 'after': ['complete']}, {'verb': 'bring', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['Alibaba'], 'after': ['a US$ 250 million Series D financing round', ',', 'bring']}, {'verb': 'bring', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['Alibaba'], 'after': ['a US$ 250 million Series D financing round', ',', 'bring']}, {'verb': 'is', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['a US$ 250 million Series D financing round'], 'after': ['complete']}, {'verb': 'is', 'prep': ['In April 2014', 'by on - demand transportation company Lyft', 'on - demand', 'by on - demand transportation company Lyft', 'on - demand', 'to $ 332'], 'before': ['a US$ 250 million Series D financing round'], 'after': ['complete']}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'verb': 'announce', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['Alibaba'], 'after': ['a $ 800 million deal']}, {'verb': 'announce', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['Alibaba'], 'after': ['a $ 800 million deal']}, {'verb': 'would', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['that'], 'after': ['till', 'in']}, {'verb': 'would', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['that'], 'after': ['till', 'in']}, {'verb': 'last', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['a $ 800 million deal'], 'after': ['till', 'in']}, {'verb': 'last', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['a $ 800 million deal'], 'after': ['till', 'in']}, {'verb': 'would', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['Alibaba'], 'after': ['the Olympic Games']}, {'verb': 'would', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['Alibaba'], 'after': ['the Olympic Games']}, {'verb': 'sponsor', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['in'], 'after': ['the Olympic Games']}, {'verb': 'sponsor', 'prep': ['In January 2017', 'till 2028', 'in where Alibaba would sponsor the Olympic Games'], 'before': ['in'], 'after': ['the Olympic Games']}]\n",
      "[{'verb': 'cite', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['Bloomberg'], 'after': ['sources', 'say']}, {'verb': 'cite', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['Bloomberg'], 'after': ['sources', 'say']}, {'verb': 'say', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['Bloomberg'], 'after': ['sources', 'say']}, {'verb': 'say', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['Bloomberg'], 'after': ['sources', 'say']}, {'verb': 'is', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['consider'], 'after': []}, {'verb': 'is', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['consider'], 'after': []}, {'verb': 'consider', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['say'], 'after': ['raise']}, {'verb': 'consider', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['say'], 'after': ['raise']}, {'verb': 'raise', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['consider'], 'after': ['$ 20 billion', 'through']}, {'verb': 'raise', 'prep': ['In May 2019', 'with the matter', 'as', 'that', 'through a second listing in Hong Kong', 'in Hong Kong'], 'before': ['consider'], 'after': ['$ 20 billion', 'through']}]\n",
      "[{'verb': 'launch', 'prep': ['In 1999', 'of Alibaba , Alibaba'], 'before': ['Jack Ma'], 'after': ['the primary business']}, {'verb': 'launch', 'prep': ['In 1999', 'of Alibaba , Alibaba'], 'before': ['Jack Ma'], 'after': ['the primary business']}]\n",
      "[{'verb': 'offer', 'prep': ['In 2003', 'of products for retail sale', 'for retail sale'], 'before': ['Alibaba'], 'after': ['Taobao Marketplace', 'offer']}, {'verb': 'offer', 'prep': ['In 2003', 'of products for retail sale', 'for retail sale'], 'before': ['Alibaba'], 'after': ['Taobao Marketplace', 'offer']}]\n",
      "[{'verb': 'introduce', 'prep': ['In April 2008'], 'before': ['Taobao'], 'after': ['a spin - off', ',', 'Taobao Mall']}, {'verb': 'introduce', 'prep': ['In April 2008'], 'before': ['Taobao'], 'after': ['a spin - off', ',', 'Taobao Mall']}]\n",
      "[{'verb': 'launch', 'prep': ['In 2010'], 'before': ['Alibaba'], 'after': ['AliExpress']}, {'verb': 'launch', 'prep': ['In 2010'], 'before': ['Alibaba'], 'after': ['AliExpress']}]\n",
      "[{'verb': 'include', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['Alibaba'], 'after': ['SF Express', ')', 'establish']}, {'verb': 'include', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['Alibaba'], 'after': ['SF Express', ')', 'establish']}, {'verb': 'establish', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['Alibaba'], 'after': ['SF Express', ')', 'establish']}, {'verb': 'establish', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['Alibaba'], 'after': ['SF Express', ')', 'establish']}, {'verb': 'call', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['a company'], 'after': ['Cainiao', 'for']}, {'verb': 'call', 'prep': ['In 2013', 'for delivery of packages in China', 'of packages', 'in China'], 'before': ['a company'], 'after': ['Cainiao', 'for']}]\n",
      "[{'verb': 'launch', 'prep': ['On 11 June 2014'], 'before': ['Alibaba'], 'after': ['US shopping site']}, {'verb': 'launch', 'prep': ['On 11 June 2014'], 'before': ['Alibaba'], 'after': ['US shopping site']}]\n",
      "[{'verb': 'announce', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['intend']}, {'verb': 'announce', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['intend']}, {'verb': 'intend', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['intend']}, {'verb': 'intend', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['intend']}, {'verb': 'acquire', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['acquire']}, {'verb': 'acquire', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['Alibaba'], 'after': ['acquire']}, {'verb': 'pay', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['acquire'], 'after': ['$ 500 million', 'for', 'and', 'buy']}, {'verb': 'pay', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['acquire'], 'after': ['$ 500 million', 'for', 'and', 'buy']}, {'verb': 'buy', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['pay'], 'after': ['$ 500 M', 'worth']}, {'verb': 'buy', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['pay'], 'after': ['$ 500 M', 'worth']}, {'verb': 'exist investors', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['from'], 'after': []}, {'verb': 'exist investors', 'prep': ['In April 2016', 'that', 'in Lazada', 'by', 'for new shares', 'of shares', 'from exist investors'], 'before': ['from'], 'after': []}]\n",
      "[{'verb': 'launch', 'prep': ['In October 2016', 'as an online mall for brands such as airline companies and agencies', 'for brands such as airline companies and agencies', 'such as airline companies and agencies'], 'before': ['Alibaba'], 'after': ['Alitrip']}, {'verb': 'launch', 'prep': ['In October 2016', 'as an online mall for brands such as airline companies and agencies', 'for brands such as airline companies and agencies', 'such as airline companies and agencies'], 'before': ['Alibaba'], 'after': ['Alitrip']}, {'verb': 'is', 'prep': ['In October 2016', 'as an online mall for brands such as airline companies and agencies', 'for brands such as airline companies and agencies', 'such as airline companies and agencies'], 'before': ['an online travel platform'], 'after': ['design']}, {'verb': 'is', 'prep': ['In October 2016', 'as an online mall for brands such as airline companies and agencies', 'for brands such as airline companies and agencies', 'such as airline companies and agencies'], 'before': ['an online travel platform'], 'after': ['design']}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'verb': 'start', 'prep': ['In 2017', 'of supermarkets'], 'before': ['open'], 'after': []}, {'verb': 'start', 'prep': ['In 2017', 'of supermarkets'], 'before': ['open'], 'after': []}, {'verb': 'open', 'prep': ['In 2017', 'of supermarkets'], 'before': ['open'], 'after': ['a chain']}, {'verb': 'open', 'prep': ['In 2017', 'of supermarkets'], 'before': ['open'], 'after': ['a chain']}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f48ce520ef9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mnoun_verbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_noun_verb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mpreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mcombined_nvp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_noun_verb_preps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun_verbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f48ce520ef9c>\u001b[0m in \u001b[0;36mget_noun_verb\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.merge\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/spacy/errors.py\u001b[0m in \u001b[0;36mdeprecation_warning\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0m_warn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deprecation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/spacy/errors.py\u001b[0m in \u001b[0;36m_warn\u001b[0;34m(message, warn_type)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwarn_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSPACY_WARNING_TYPES\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWARNINGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwarn_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSPACY_WARNING_FILTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/inspect.py\u001b[0m in \u001b[0;36mismodule\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0m__doc__\u001b[0m         \u001b[0mdocumentation\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         __file__        filename (missing for built-in modules)\"\"\"\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "\n",
    "### Trys to extract nouns and verbs with preposition phrases based on the token dependency tree from pos tagging ###\n",
    "# Searches dependency tree and if a verb is found, will then traverse the tree left and right\n",
    "# to determine the relationship in the sentence\n",
    "# will also try and find the preopsitional phrase in the sentence if any and try to add that to the relationship as well\n",
    "\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "# prep = description of edge\n",
    "\n",
    "def get_preps(doc):\n",
    "    preps = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADP':\n",
    "            pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            preps.append(pp)\n",
    "    return preps\n",
    "\n",
    "def get_noun_verb(doc):\n",
    "    \n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            \n",
    "            subject = [w.text for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            after = [w.text for w in token.head.rights]\n",
    "            \n",
    "            if len(subject) == 0:\n",
    "                subject = [token.head.text]\n",
    "                after = [w.text for w in token.rights]\n",
    "            \n",
    "            relation_dict = {\n",
    "                'before': [],\n",
    "                'after': [],\n",
    "                'verb': token.text,\n",
    "                'prep': []\n",
    "            }\n",
    "            \n",
    "            relation_dict['before'].extend(subject)\n",
    "            relation_dict['after'].extend(after)\n",
    "            \n",
    "            relations.append(relation_dict)\n",
    "            \n",
    "    return relations\n",
    "\n",
    "def combine_noun_verb_preps(noun_verbs, preps):\n",
    "    holder = []\n",
    "    \n",
    "    for nv in noun_verbs:\n",
    "        \n",
    "        # will apply before preposition and after preposition\n",
    "        holder.append(nv)\n",
    "        nv['prep'].extend(preps)\n",
    "        holder.append(nv)\n",
    "    \n",
    "    return holder\n",
    "\n",
    "KEY = 'Method1'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method1\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            noun_verbs = get_noun_verb(doc)\n",
    "            preps = get_preps(doc)\n",
    "            combined_nvp = combine_noun_verb_preps(noun_verbs, preps)\n",
    "            print (combined_nvp)\n",
    "            break\n",
    "            total_d[u][KEY].append(combined_nvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "### Similar to method 1 ###\n",
    "# Extracts noun chunks and verbs and creates span of nouns and verbs \n",
    "# spans are nouns before a verb, verb, and then nouns after\n",
    "# For example sentence may be: nouns1 + verb1 + nouns2 + verb2 + nouns3 \n",
    "# the spans will be:\n",
    "# - nouns1, verb1, nouns2\n",
    "# - nouns2, verb2, nouns3\n",
    "\n",
    "# theyre all placed in json and is in a similar style to method 1\n",
    "# before = node 1 \n",
    "# after = node 2\n",
    "# verb = edge from node 1 -> node 2\n",
    "\n",
    "def get_nouns(doc):\n",
    "    nouns = []\n",
    "    noun = textacy.extract.noun_chunks(doc)\n",
    "\n",
    "    for n in noun:\n",
    "\n",
    "        end = n.end\n",
    "        n = str(n)\n",
    "        temp_word = []\n",
    "        for ch in n:\n",
    "            if ch.isalpha() or ch.isnumeric() or ch is ' ':\n",
    "                temp_word.append(ch)\n",
    "\n",
    "        nouns.append((''.join(temp_word), end))\n",
    "        \n",
    "    return nouns\n",
    "\n",
    "def get_verbs(doc):\n",
    "    main_verbs = textacy.spacier.utils.get_main_verbs_of_sent(doc)\n",
    "    verbs = []\n",
    "    for verb in main_verbs:\n",
    "\n",
    "        verb_index = verb.i\n",
    "        verbs.append((verb.text, verb_index))\n",
    "\n",
    "    tbd = []\n",
    "    for i in range(len(verbs)):\n",
    "        v = verbs[i]\n",
    "\n",
    "        for n in nouns:\n",
    "            if v[0] in n[0]:\n",
    "                tbd.append(v)\n",
    "\n",
    "    cleaned_verbs = []\n",
    "    for v in verbs:\n",
    "        if v in tbd:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_verbs.append(v)\n",
    "\n",
    "    return cleaned_verbs\n",
    "\n",
    "def span_relation(nouns, verbs):\n",
    "    relations = []\n",
    "\n",
    "    # ensure we can collect appropriate spans by setting bounds\n",
    "    before = 0\n",
    "    end = float('inf')\n",
    "\n",
    "    for i in range(len(verbs)):\n",
    "\n",
    "        v = verbs[i]\n",
    "\n",
    "        # helper variables to handle span creation\n",
    "        if i == 0:\n",
    "            before = float('-inf')\n",
    "        else:\n",
    "            before = i - 1\n",
    "\n",
    "        if i == len(verbs) - 1:\n",
    "            after = float('inf')\n",
    "        else:\n",
    "            after = i + 1\n",
    "\n",
    "        relation_dict = {\n",
    "            'before': [],\n",
    "            'verb': v[0],\n",
    "            'after': []\n",
    "        }\n",
    "\n",
    "        # creating spans of noun chunks and verbs\n",
    "        for n in nouns:\n",
    "\n",
    "            if before == float('-inf'):\n",
    "                if n[1] <= verbs[0][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "            else:\n",
    "                if n[1] < v[1] and n[1] > verbs[before][1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "                elif n[1] == v[1]:\n",
    "                    relation_dict['before'].append(n[0])\n",
    "\n",
    "        for n in nouns:\n",
    "\n",
    "            if after == float('inf'):\n",
    "                if n[1] > verbs[-1][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "            else:\n",
    "                if n[1] > v[1] and n[1] <= verbs[after][1]:\n",
    "                    relation_dict['after'].append(n[0])\n",
    "\n",
    "        relations.append(relation_dict)\n",
    "        \n",
    "    return relations\n",
    "\n",
    "KEY = 'Method2'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # creating a separate key for Method2\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            nouns = get_nouns(doc)\n",
    "            verbs = get_verbs(doc)\n",
    "            sr = span_relation(nouns, verbs)\n",
    "            \n",
    "            total_d[u][KEY].append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Method\n",
    "\n",
    "# speciify cues (currently set as 'be' or 'is')\n",
    "# and will grab those relationships\n",
    "\n",
    "KEY = 'Attributes'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # using the raw form\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                # cue - currently set as default ('be')\n",
    "                # can be modified for different verbs\n",
    "                relationship = textacy.extract.semistructured_statements(doc, ent.text)\n",
    "                for r in relationship:\n",
    "                    \n",
    "                    l = []\n",
    "                    for words in r:\n",
    "                        l.append(str(words))\n",
    "                        \n",
    "                    total_d[u][KEY].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money\n",
    "\n",
    "# taken from spacy website\n",
    "# added the rest of the spans to the beginning of the relation as well\n",
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result\n",
    "\n",
    "def extract_currency_relations(doc):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "    \n",
    "    str_noun_chunks = []\n",
    "    for n in list(doc.noun_chunks):\n",
    "        str_noun_chunks.append(str(n))\n",
    "        \n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w.text for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                # subject = subject[0]\n",
    "                relations.append((subject, money.text, list(str_noun_chunks)))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head.text, money.text))\n",
    "    return relations\n",
    "\n",
    "\n",
    "KEY = 'Money'\n",
    "# EACH URL TAKES ABOUT 1 MINUTE TO RUN\n",
    "for u in urls:\n",
    "    \n",
    "    # using the raw form\n",
    "    total_d[u][KEY] = []\n",
    "    article = resolved_d[u]\n",
    "    \n",
    "    for p in article:\n",
    "        # list is broken up into paragraphs\n",
    "        # will then break up the paragraph into sentences\n",
    "        sentences = p.split('.')\n",
    "        for s in sentences:\n",
    "            doc = nlp(s)\n",
    "            \n",
    "            money_data = extract_currency_relations(doc)\n",
    "            if len(money_data) > 0:\n",
    "                total_d[u][KEY].append(money_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick output of everything\n",
    "import json\n",
    "\n",
    "\n",
    "for k in total_d.keys():\n",
    "    \n",
    "    filename = '{0}.json'.format(k)\n",
    "    fil = 'raw_' + filename.split('/')[-1]\n",
    "    \n",
    "    with open(fil, 'w') as outfile:\n",
    "        json.dump(total_d[k], outfile, sort_keys=True, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
